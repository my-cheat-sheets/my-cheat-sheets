<!DOCTYPE html>
<html lang="en">
<head><meta charset="UTF-8"><title>PySpark Complete Cheatsheet 2025</title></head>
<body>
<h1>‚ö° PySpark Complete Cheatsheet 2025</h1>

<details open>
<summary>üöÄ PySpark - Setup & Initialization</summary>
<ul>
<ol>Basic SparkSession<code>from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()</code></ol>
<ol>Full Config<code>spark = SparkSession.builder \
    .appName("PySparkApp") \
    .master("local[*]") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.cores", "2") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()</code></ol>
<ol>Stop Spark<code>spark.stop()</code></ol>
</ul>
</details>

<details>
<summary>üìñ Core Imports & DataTypes</summary>
<ul>
<ol>Imports<code>from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.sql import DataFrame</code></ol>
<ol>DataTypes<code># Primitives
IntegerType(), LongType(), FloatType(), DoubleType()
StringType(), BooleanType(), TimestampType(), DateType()

# Complex
StructType([StructField("name", StringType()), StructField("age", IntegerType())])
ArrayType(StringType())
MapType(StringType(), IntegerType())
StructType(fields=[
    StructField("address", StructType([
        StructField("city", StringType()),
        StructField("zip", StringType())
    ]))
])</code></ol>
</ul>
</details>

<details>
<summary>üìÅ Reading & Writing Data</summary>
<ul>
<ol>CSV<code>df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .option("delimiter", ",") \
    .csv("data.csv")

df.write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("output.csv")</code></ol>
<ol>Parquet (Fastest)<code>df = spark.read.parquet("data.parquet")
df.write.mode("overwrite").parquet("output.parquet")</code></ol>
<ol>JSON<code>df = spark.read.option("multiline", "true").json("data.json")
df.write.mode("overwrite").json("output.json")</code></ol>
<ol>JDBC (Database)<code>df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://host/db") \
    .option("dbtable", "table") \
    .option("user", "user") \
    .option("password", "pass") \
    .load()</code></ol>
<ol>All Formats<code># Read
spark.read.format("avro").load("data.avro")
spark.read.format("orc").load("data.orc")
spark.read.text("data.txt")
spark.read.table("hive_table")

# Write
df.write.format("delta").mode("overwrite").save("delta_table")
df.write.format("iceberg").mode("overwrite").save("iceberg_table")</code></ol>
</ul>
</details>

<details>
<summary>üîç DataFrame Operations</summary>
<ul>
<ol>Inspect<code>df.printSchema()
df.show(20, truncate=False)
df.describe().show()
df.columns
df.dtypes
df.count()
df.cache()
df.explain()</code></ol>
<ol>Select/Columns<code>df.select("name", "age")
df.select(col("name"), col("age"))
df.select("*").filter(col("age") > 25)
df.drop("column1", "column2")
df.withColumnRenamed("old", "new")</code></ol>
<ol>Filter/Where<code>df.filter(col("age") > 25)
df.filter((col("age") > 25) & (col("city") == "NYC"))
df.where("age > 25 AND city = 'NYC'")
df.filter(df.age > 25)</code></ol>
</ul>
</details>

<details>
<summary>‚ú® SQL Functions (500+)</summary>
<ul>
<ol>Math<code>round(col("price"), 2)
ceil(col("price")), floor(col("price"))
abs(col("value")), sqrt(col("value"))
rand(), randn()</code></ol>
<ol>String<code>upper(col("name")), lower(col("name"))
length(col("name")), substring(col("name"), 1, 3)
concat(col("first"), lit("_"), col("last"))
regexp_replace(col("email"), "@", "_")</code></ol>
<ol>Date/Time<code>current_date(), current_timestamp()
to_date(col("date_str"), "yyyy-MM-dd")
date_format(col("date"), "yyyy-MM-dd")
datediff(col("end"), col("start"))
months_between(col("date1"), col("date2"))</code></ol>
<ol>Aggregate<code>count("*"), count("col")
sum("sales"), avg("price"), min("value"), max("value")
stddev("price"), variance("price")
collect_list("name"), collect_set("name")</code></ol>
<ol>Window<code>from pyspark.sql.window import Window
windowSpec = Window.partitionBy("dept").orderBy("salary")
rank().over(windowSpec)
row_number().over(windowSpec)
lag("salary", 1).over(windowSpec)</code></ol>
</ul>
</details>

<details>
<summary>‚öôÔ∏è GroupBy & Aggregations</summary>
<ul>
<ol>Basic<code>df.groupBy("department").count()
df.groupBy("department").agg(sum("salary").alias("total_salary"))</code></ol>
<ol>Multiple<code>df.groupBy("dept", "year") \
    .agg(
        sum("sales").alias("total_sales"),
        avg("profit").alias("avg_profit"),
        count("*").alias("record_count")
    )</code></ol>
<ol>Pivot<code>df.groupBy("year").pivot("department").sum("sales").show()</code></ol>
<ol>Cube/Rollup<code>df.groupBy("dept").cube("year").sum("sales")
df.groupBy("dept").rollup("year").sum("sales")</code></ol>
</ul>
</details>

<details>
<summary>üîó Joins (All Types)</summary>
<ul>
<ol>Basic<code>df1.join(df2, "id", "inner")
df1.join(df2, df1.id == df2.id, "inner")
df1.join(df2, col("df1.id") == col("df2.id"), "left")</code></ol>
<ol>All Types<code># inner, left, right, outer, left_anti, left_semi, cross
df1.join(df2, "id", "left_outer")
df1.join(df2, "id", "right_outer")
df1.join(df2, "id", "full_outer")</code></ol>
<ol>Broadcast Join (Small tables)<code>from pyspark.sql.functions import broadcast
df1.join(broadcast(df2), "id")</code></ol>
</ul>
</details>

<details>
<summary>üìä UDFs & Performance</summary>
<ul>
<ol>Pandas UDF (Vectorized)<code>from pyspark.sql.functions import pandas_udf
@pandas_udf("double")
def pandas_plus_one(v: pd.Series) -> pd.Series:
    return v + 1
df.withColumn("new_col", pandas_plus_one(df.col))</code></ol>
<ol>Regular UDF<code>from pyspark.sql.functions import udf
double_udf = udf(lambda x: x * 2, DoubleType())
df.withColumn("doubled", double_udf(df.value))</code></ol>
<ol>Configs (Performance)<code># Memory & Partitioning
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.shuffle.partitions", "200")
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")

# Arrow Optimization
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", "10000")</code></ol>
</ul>
</details>

<details>
<summary>üóÑÔ∏è Delta Lake (ACID Tables)</summary>
<ul>
<ol>Basic<code>df.write.format("delta").mode("overwrite").save("delta_table")
spark.read.format("delta").load("delta_table")</code></ol>
<ol>Operations<code># Time Travel
spark.read.format("delta").option("versionAsOf", 5).load("table")

# Merge (UPSERT)
delta_table = DeltaTable.forPath(spark, "delta_table")
delta_table.alias("target") \
    .merge(updates.alias("source"), "target.id = source.id") \
    .whenMatchedUpdateAll() \
    .whenNotMatchedInsertAll() \
    .execute()</code></ol>
</ul>
</details>

<details>
<summary>üåä Structured Streaming (Modern)</summary>
<ul>
<li>Basic Stream<code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Streaming").getOrCreate()

# Read stream
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "topic1") \
    .load()

# Process
processed = df.selectExpr("CAST(value AS STRING)", "timestamp")

# Write stream
query = processed.writeStream \
    .outputMode("append") \
    .format("console") \
    .trigger(processingTime="10 seconds") \
    .start()

query.awaitTermination()</code></li>
<li>Output Modes<code># append (default) - new rows only
# complete - all results
# update - changed rows only
query = df.writeStream.outputMode("complete").start()</code></li>
<li>Triggers<code># ProcessingTime
.trigger(processingTime="10 seconds")
.trigger(processingTime='1 minute')

# Once (batch)
.trigger(once=True)

# Continuous (experimental)
.trigger(continuous="1 second")</code></li>
<li>Watermark (Late Data)<code>df.withWatermark("event_time", "10 minutes") \
    .groupBy(window("event_time", "10 minutes")) \
    .count()</code></li>
<li>Windows<code># Tumbling (fixed)
.groupBy(window(col("timestamp"), "10 minutes"))

# Sliding
.groupBy(window(col("timestamp"), "10 minutes", "5 minutes"))

# Session
.groupBy(session_window("timestamp", "10 minutes"))</code></li>
<li>Kafka Sink<code>df.writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "output-topic") \
    .start()</code></li>
<li>File Sink<code>df.writeStream \
    .format("parquet") \
    .option("path", "/path/to/output") \
    .option("checkpointLocation", "/path/checkpoint") \
    .start()</code></li>
</ul>
</details>

<details>
<summary>üì° Sources & Sinks</summary>
<ul>
<li>Sources<code># Files
spark.readStream.format("csv").load("input/")
spark.readStream.text("input/")
spark.readStream.parquet("input/")

# Kafka
.option("kafka.bootstrap.servers", "host:9092")
.option("subscribe", "topic1,topic2")

# Socket
.format("socket").option("host", "localhost").option("port", 9999)

# Rate (testing)
.format("rate").option("rowsPerSecond", 10)</code></li>
<li>Sinks<code># Console (debug)
.format("console")

# Memory (testing)
.format("memory").queryName("test_table")

# Files
.format("parquet").option("path", "output/")

# Kafka
.format("kafka")

# ForeachBatch (custom)
.foreachBatch(lambda batch_df, batch_id: batch_df.write.parquet(f"output/batch_{batch_id}"))

# Delta
.format("delta").option("checkpointLocation", "/checkpoint")</code></li>
</ul>
</details>

<details>
<summary>‚ö° Legacy DStreams (Deprecated)</summary>
<ul>
<li>Basic<code>from pyspark.streaming import StreamingContext
ssc = StreamingContext(spark.sparkContext, 2)  # 2s batches

# Socket stream
lines = ssc.socketTextStream("localhost", 9999)
words = lines.flatMap(lambda line: line.split(" "))
wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

wordCounts.pprint()
ssc.start()
ssc.awaitTermination()</code></li>
<li>Warning<code># ‚ö†Ô∏è LEGACY - Use Structured Streaming instead
# DStreams deprecated in Spark 3.5+, removed in Spark 4.0</code></li>
</ul>
</details>

<details>
<summary>üîÑ State & Fault Tolerance</summary>
<ul>
<li>State Store<code># MapWithState (Structured Streaming)
from pyspark.sql.functions import *
state_df = df.withWatermark("timestamp", "10 minutes") \
    .groupByKey(lambda x: x.user_id) \
    .mapGroupsWithState(...)</code></li>
<li>Checkpointing<code># ALWAYS required for fault tolerance
.option("checkpointLocation", "/path/to/checkpoint")

# Exactly-once semantics
spark.conf.set("spark.sql.streaming.statefulOperator.checkCorrectness.enabled", "true")</code></li>
<li>Multiple Streams<code># Fanout
query1 = stream1.writeStream.start()
query2 = stream1.alias("s1").join(stream2).writeStream.start()</code></li>
</ul>
</details>

<details>
<summary>üìà Streaming Production Configs</summary>
<ul>
<li>Micro-batch<code>spark.conf.set("spark.sql.streaming.pollingDelay", "0")
spark.conf.set("spark.sql.streaming.minBatchesToRetain", "100")
spark.conf.set("spark.sql.adaptive.enabled", "true")</code></li>
<li>Memory<code>spark.conf.set("spark.sql.streaming.stateStore.minDeltasForFullState", "5")
spark.conf.set("spark.sql.streaming.statefulOperator.useSharedMemory", "false")</code></li>
<li>Kafka<code>spark.conf.set("spark.sql.streaming.metricsEnabled", "true")
.option("maxOffsetsPerTrigger", 1000)
.option("minOffsetsPerTrigger", 500)</code></li>
</ul>
</details>


<details>
<summary>‚ö° Spark SQL & Optimization</summary>
<ul>
<ol>Register Table<code>df.createOrReplaceTempView("my_table")
spark.sql("SELECT * FROM my_table WHERE age > 25").show()</code></ol>
<ol>Advanced SQL<code># Window Functions
spark.sql("""
    SELECT 
        dept, 
        salary,
        rank() OVER (PARTITION BY dept ORDER BY salary DESC) as salary_rank
    FROM employees
""").show()</code></ol>
<ol>Cache/Persist<code>df.cache()  # MEMORY_ONLY
df.persist(StorageLevel.MEMORY_AND_DISK)
df.unpersist()</code></ol>
</ul>
</details>

<details>
<summary>üîß All Spark Configs (Production)</summary>
<table border="1" style="border-collapse:collapse;">
<tr><th>Config</th><th>Type</th><th>Default</th><th>Purpose</th></tr>
<tr><td>spark.sql.adaptive.enabled</td><td>boolean</td><td>false</td><td>Adaptive Query Execution</td></tr>
<tr><td>spark.sql.shuffle.partitions</td><td>int</td><td>200</td><td>Shuffle partitions</td></tr>
<tr><td>spark.executor.memory</td><td>string</td><td>1g</td><td>Executor memory</td></tr>
<tr><td>spark.executor.cores</td><td>int</td><td>1</td><td>Executor cores</td></tr>
<tr><td>spark.sql.execution.arrow.pyspark.enabled</td><td>boolean</td><td>false</td><td>Arrow optimization</td></tr>
<tr><td>spark.serializer</td><td>string</td><td>JavaSerializer</td><td>KryoSerializer (faster)</td></tr>
<tr><td>spark.sql.adaptive.coalescePartitions.enabled</td><td>boolean</td><td>true</td><td>Auto coalesce</td></tr>
</table>
</details>

</body>
</html>
